{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting XML files for Ross Travel 389"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile (\"Fares Data/Ross Travel_389/remote_dataset_11006_2023-07-06_04-02-50.zip\", 'r') as zObject:\n",
    "\n",
    "    zObject.extractall(path=\"Fares Data\\Ross Travel_389\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Parsing Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_xml(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    return root\n",
    "\n",
    "\n",
    "def extract_data(root):\n",
    "    data = []\n",
    "    for record in root.findall('ScheduledStopPoint'):\n",
    "        row = {}\n",
    "        for field in record:\n",
    "            row[field.tag] = field.text\n",
    "        data.append(row)\n",
    "    return data\n",
    "\n",
    "def to_dataframe(data):\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def tag_attrib(root):\n",
    "    data = []\n",
    "    for x in root:\n",
    "        y = x.tag\n",
    "        data.append(y)\n",
    "    return data\n",
    "\n",
    "def Description(root):\n",
    "    data = []\n",
    "    for x in root:\n",
    "        y = x.text\n",
    "        data.append(y)\n",
    "    return data\n",
    "\n",
    "\n",
    "def main(xml_file):\n",
    "    root = parse_xml(xml_file)\n",
    "    print(root)\n",
    "    data = extract_data(root)\n",
    "    print(data)\n",
    "    df = to_dataframe(data)\n",
    "    print(df)\n",
    "    ta = tag_attrib(root)\n",
    "    print(ta)\n",
    "    data_test = Description(root)\n",
    "    print(data_test)   \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main('Fares Data/Ross Travel_389/ROSS_144_Inbound_ADsgl_43ad4e3d-65a5-49b7-8a29-3dceda1d9905_638241799544014471.xml')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new file based on the Fares Data directory for saving all JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fares Data JSON'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil, os\n",
    "\n",
    "def ig_f(dir, files):\n",
    "    return [f for f in files if os.path.isfile(os.path.join(dir, f))]\n",
    "\n",
    "SRC = 'Fares Data'\n",
    "\n",
    "DEST = 'Fares Data JSON'\n",
    "\n",
    "shutil.copytree(SRC, DEST, ignore=ig_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all the xml from their zip files in 'Fare Data' folder and remove an .zip files afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, zipfile\n",
    "\n",
    "my_dir = 'C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS/Fares Data'\n",
    "\n",
    "for subdir, dirs, files in os.walk(my_dir):\n",
    "    for file in files:\n",
    "        #print os.path.join(subdir, file)\n",
    "        filepath = subdir + os.sep + file\n",
    "\n",
    "        if filepath.endswith(\".zip\"):\n",
    "            zip_ref = zipfile.ZipFile(filepath) # create zipfile object\n",
    "            zip_ref.extractall(subdir) # extract file to dir\n",
    "            zip_ref.close() # close file\n",
    "            os.remove(filepath) # delete zipped file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Scrape through the Fares Data Files after they've been converted from .zip to .xml\n",
    " - Check and see if there are any folders that have subfolders\n",
    " - grab a list of these folders\n",
    " - move any .xml files from these subfolders to the parent folder\n",
    " - delete the subfolders\n",
    " - Fares Data folder now ready for JSON conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from os import listdir, rmdir\n",
    "from shutil import move\n",
    "import pathlib\n",
    "\n",
    "rootdir = 'C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS/Fares Data/'\n",
    "\n",
    "\n",
    "def listdirs(rootdir):\n",
    "\n",
    "    list_of_subdir = []\n",
    "\n",
    "    for file in os.listdir(rootdir):\n",
    "        d = os.path.join(rootdir, file)\n",
    "        for file2 in os.listdir(d):\n",
    "            e = os.path.join(d, file2)\n",
    "            if os.path.isdir(e):\n",
    "                list_of_subdir.append(e)\n",
    "                #listdirs(e)\n",
    "    return list_of_subdir\n",
    "\n",
    "def move_xml(sublist):\n",
    "\n",
    "    for i in sublist:\n",
    "        path = pathlib.Path(str(i))\n",
    "        dest = path.parent\n",
    "        for filename in os.listdir(path):\n",
    "            new_src = str(path) + os.sep + str(filename)\n",
    "            new_dst = str(dest)\n",
    "            print(new_dst)\n",
    "            if new_src.endswith(\".xml\"):\n",
    "                move(new_src, new_dst)\n",
    "\n",
    "def del_subs(sublist):\n",
    "\n",
    "    for i in sublist:\n",
    "        os.rmdir(i)\n",
    "\n",
    " \n",
    "sublist = listdirs(rootdir)\n",
    "print(sublist)\n",
    "# move_xml(sublist)\n",
    "del_subs(sublist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XML to JSON testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmltodict, json\n",
    "\n",
    "with open('Fares Data/Ross Travel_389/ROSS_144_Inbound_ADsgl_43ad4e3d-65a5-49b7-8a29-3dceda1d9905_638241799544014471.xml','r') as test_file:\n",
    "    obj = xmltodict.parse(test_file.read())\n",
    "\n",
    "x = json.dumps(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, xmltodict, json\n",
    "\n",
    "SRC = 'C:/Users/Ross/Documents/BODS Project/BODS/Ross Travel'\n",
    "\n",
    "for subdir, dirs, files in os.walk(SRC):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        if filepath.endswith(\".xml\"):\n",
    "            with open(filepath,'r') as test_file:\n",
    "                obj = xmltodict.parse(test_file.read())\n",
    "                jd = json.dumps(obj, ensure_ascii=False, indent=4)\n",
    "                new_filepath = filepath.replace('Ross Travel', 'Ross Travel JSON')\n",
    "                final_filepath = new_filepath.replace('.xml', '.json')\n",
    "                with open(final_filepath, \"w\", encoding='utf-8') as jf:\n",
    "                    jf.write(jd)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, zipfile\n",
    "\n",
    "my_dir = 'C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS/Fares Data JSON'\n",
    "\n",
    "for subdir, dirs, files in os.walk(my_dir):\n",
    "    for file in files:\n",
    "        #print os.path.join(subdir, file)\n",
    "        filepath = subdir + os.sep + file\n",
    "\n",
    "        if filepath.endswith(\".json\"):\n",
    "            os.remove(filepath) # delete zipped file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Quick SubScript to test out timetable data ###\n",
    "\n",
    "import os, xmltodict, json\n",
    "\n",
    "SRC = 'C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS/Arriva Donnington Timetable Data'\n",
    "\n",
    "for subdir, dirs, files in os.walk(SRC):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        if filepath.endswith(\".xml\"):\n",
    "            with open(filepath,'r') as test_file:\n",
    "                obj = xmltodict.parse(test_file.read())\n",
    "                jd = json.dumps(obj, ensure_ascii=False, indent=4)\n",
    "                new_filepath = filepath.replace('Arriva Donnington Timetable Data', 'Arriva JSON')\n",
    "                final_filepath = new_filepath.replace('.xml', '.json')\n",
    "                jf = open(final_filepath, \"w\", encoding='utf-8')\n",
    "                jf.write(jd)\n",
    "                jf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inital Python code for scraping through the Timetable JSON and extracting the nesscessary geo data for Michael to use.\n",
    "\n",
    "Error comes up as some JSON have different schemas, so code breaks when it runs into that error.\n",
    "\n",
    "Putting on hold for now until we have our meeting with Spencer qbout the best approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import natsort as ns\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "def dictionaries(json_dict):\n",
    "\n",
    "    '''\n",
    "    Function that grabs the appropraite dictionaries to pass through any of the remaining functions\n",
    "\n",
    "    Dictionary names are not passed through as a parameter for now for testing purposes\n",
    "\n",
    "    '''\n",
    "\n",
    "    stop_dict = json_dict['TransXChange']['StopPoints']['AnnotatedStopPointRef']\n",
    "    route_dict = json_dict['TransXChange']['RouteSections']['RouteSection']\n",
    "\n",
    "    return stop_dict, route_dict\n",
    "\n",
    "\n",
    "def stop_conversion(dict):\n",
    "\n",
    "    '''\n",
    "    Convert the stop route dictionary within timetable data and convert it to a DF\n",
    "\n",
    "    '''\n",
    "\n",
    "    df = pd.DataFrame.from_dict(dict)\n",
    "    temp_df = pd.json_normalize(df['Location'])\n",
    "    df = df.drop('Location', axis=1)\n",
    "    final_df = pd.concat([df, temp_df],axis=1)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "def route_conversion(dict):\n",
    "\n",
    "    '''\n",
    "    Convert the route dictionary within timetable data and convert it to a DF\n",
    "    \n",
    "    '''\n",
    "\n",
    "    df = pd.DataFrame.from_dict(dict)\n",
    "    final_route_df = []\n",
    "\n",
    "    for i in range(0, len(df)):\n",
    "        x = list(df['RouteLink'].iloc[i])\n",
    "        temp_df1 = pd.DataFrame.from_dict(x)\n",
    "        temp_df2 = pd.json_normalize(temp_df1['From'])\n",
    "        temp_df2.rename(columns={'StopPointRef': 'From'}, inplace=True)\n",
    "        temp_df3 = pd.json_normalize(temp_df1['To'])\n",
    "        temp_df3.rename(columns={'StopPointRef': 'To'}, inplace=True)\n",
    "        temp_df1 = temp_df1.drop(['From', 'To'], axis=1)\n",
    "        temp_df4 = pd.concat([temp_df1, temp_df2],axis=1)\n",
    "        final_df = pd.concat([temp_df4, temp_df3],axis=1)\n",
    "        final_df['Group @id'] = df.loc[i,'@id']\n",
    "        first_column = final_df.pop('Group @id')\n",
    "        final_df.insert(0, 'Group @id', first_column)\n",
    "        final_route_df.append(final_df)\n",
    "    \n",
    "    final_route_df = pd.concat(final_route_df).reset_index(drop=True)\n",
    "\n",
    "    return final_route_df\n",
    "\n",
    "def df_merge(route_df, stop_df):\n",
    "\n",
    "    '''\n",
    "    Merging the DF's a cleaning before exporting to CSV\n",
    "    \n",
    "    '''\n",
    "\n",
    "    temp_df1 = route_df.merge(stop_df, left_on='From', right_on='StopPointRef')\n",
    "    final_df = temp_df1.merge(stop_df, left_on='To', right_on='StopPointRef')\n",
    "    final_df = final_df[['Group @id', '@id', 'From', 'CommonName_x', 'Longitude_x', 'Latitude_x', 'To', 'CommonName_y', 'Longitude_y', 'Latitude_y']]\n",
    "    \n",
    "    final_df['@id'] = pd.Categorical(final_df['@id'], ordered=True, categories= ns.natsorted(final_df['@id'].unique()))\n",
    "    final_df = final_df.sort_values('@id')\n",
    "\n",
    "    final_df.rename(columns={'Group @id': 'Grouped Route IDs',\n",
    "                             '@id': 'Route ID',\n",
    "                             'From': 'From ID',\n",
    "                             'CommonName_x':'From - Common Name',\n",
    "                             'Longitude_x': 'From - Longitude',\n",
    "                             'Latitude_x': 'From - Latitude',\n",
    "                             'To': 'To ID',\n",
    "                             'CommonName_y':'To - Common Name',\n",
    "                             'Longitude_y': 'To - Longitude',\n",
    "                             'Latitude_y': 'To - Latitude',}, inplace=True)\n",
    "\n",
    "    final_df.reset_index(drop=True)\n",
    "    \n",
    "    return final_df\n",
    "    \n",
    "    \n",
    "def main(json_folder):\n",
    "\n",
    "    \n",
    "    for filename in os.listdir(json_folder):\n",
    "        filepath = os.path.join(json_folder, filename)\n",
    "        print(filepath)\n",
    "        with open(filepath) as test_json:\n",
    "            json_dict = json.load(test_json)\n",
    "            stop_dict, route_dict = dictionaries(json_dict)\n",
    "            stop_df = stop_conversion(stop_dict)\n",
    "            route_df = route_conversion(route_dict)\n",
    "            merged_df = df_merge(route_df, stop_df)\n",
    "            merged_df.to_csv(os.path.join('C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS/Arriva Donnington CSV/', (str(filename) + '.csv')))\n",
    "                        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main('C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS/Arriva Donnington JSON/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, xmltodict, json\n",
    "\n",
    "SRC = 'C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS/Fares Data/Ross Travel_389'\n",
    "\n",
    "\n",
    "for subdir, dirs, files in os.walk(SRC):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        if filepath.endswith(\".xml\"):\n",
    "            with open(filepath,'r') as test_file:\n",
    "                obj = xmltodict.parse(test_file.read())\n",
    "                jd = json.dumps(obj, ensure_ascii=False, indent=4)\n",
    "                new_filepath = 'C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS/Ross Travel JSON/' + file\n",
    "                final_filepath = new_filepath.replace('.xml', '.json')\n",
    "                jf = open(final_filepath, \"w\", encoding='utf-8')\n",
    "                jf.write(jd)\n",
    "                jf.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\rosshamilton\\AppData\\Local\\Continuum\\anaconda3\\envs\\BODS\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:391\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_range\u001b[39m.\u001b[39;49mindex(new_key)\n\u001b[0;32m    392\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "\u001b[1;31mValueError\u001b[0m: 5 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m x \u001b[39m=\u001b[39m json_dict[\u001b[39m'\u001b[39m\u001b[39mPublicationDelivery\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mdataObjects\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mCompositeFrame\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      7\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame\u001b[39m.\u001b[39mfrom_dict(x)\n\u001b[1;32m----> 9\u001b[0m x \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mloc[\u001b[39m5\u001b[39;49m]\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(x)\n",
      "File \u001b[1;32mc:\\Users\\rosshamilton\\AppData\\Local\\Continuum\\anaconda3\\envs\\BODS\\Lib\\site-packages\\pandas\\core\\indexing.py:1073\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1070\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m   1072\u001b[0m maybe_callable \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[1;32m-> 1073\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(maybe_callable, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32mc:\\Users\\rosshamilton\\AppData\\Local\\Continuum\\anaconda3\\envs\\BODS\\Lib\\site-packages\\pandas\\core\\indexing.py:1312\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1310\u001b[0m \u001b[39m# fall thru to straight lookup\u001b[39;00m\n\u001b[0;32m   1311\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m-> 1312\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_label(key, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32mc:\\Users\\rosshamilton\\AppData\\Local\\Continuum\\anaconda3\\envs\\BODS\\Lib\\site-packages\\pandas\\core\\indexing.py:1260\u001b[0m, in \u001b[0;36m_LocIndexer._get_label\u001b[1;34m(self, label, axis)\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_label\u001b[39m(\u001b[39mself\u001b[39m, label, axis: \u001b[39mint\u001b[39m):\n\u001b[0;32m   1259\u001b[0m     \u001b[39m# GH#5567 this will fail if the label is not present in the axis.\u001b[39;00m\n\u001b[1;32m-> 1260\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49mxs(label, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32mc:\\Users\\rosshamilton\\AppData\\Local\\Continuum\\anaconda3\\envs\\BODS\\Lib\\site-packages\\pandas\\core\\generic.py:4056\u001b[0m, in \u001b[0;36mNDFrame.xs\u001b[1;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[0;32m   4054\u001b[0m             new_index \u001b[39m=\u001b[39m index[loc]\n\u001b[0;32m   4055\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 4056\u001b[0m     loc \u001b[39m=\u001b[39m index\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   4058\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(loc, np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m   4059\u001b[0m         \u001b[39mif\u001b[39;00m loc\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mbool_:\n",
      "File \u001b[1;32mc:\\Users\\rosshamilton\\AppData\\Local\\Continuum\\anaconda3\\envs\\BODS\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:393\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_range\u001b[39m.\u001b[39mindex(new_key)\n\u001b[0;32m    392\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 393\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m    395\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 5"
     ]
    }
   ],
   "source": [
    "d = 'C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS/Ross Travel JSON/ROSS_144_Inbound_ADsgl_43ad4e3d-65a5-49b7-8a29-3dceda1d9905_638241799544014471.json'\n",
    "\n",
    "with open(d) as test_json:\n",
    "    json_dict = json.load(test_json)\n",
    "\n",
    "x = json_dict['PublicationDelivery']['dataObjects']['CompositeFrame']\n",
    "df = pd.DataFrame.from_dict(x)\n",
    "\n",
    "x = df.loc[5]\n",
    "\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BODS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
