{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting XML files for Ross Travel 389"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile (\"Fares Data/Ross Travel_389/remote_dataset_11006_2023-07-06_04-02-50.zip\", 'r') as zObject:\n",
    "\n",
    "    zObject.extractall(path=\"Fares Data\\Ross Travel_389\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Parsing Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_xml(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    return root\n",
    "\n",
    "\n",
    "def extract_data(root):\n",
    "    data = []\n",
    "    for record in root.findall('ScheduledStopPoint'):\n",
    "        row = {}\n",
    "        for field in record:\n",
    "            row[field.tag] = field.text\n",
    "        data.append(row)\n",
    "    return data\n",
    "\n",
    "def to_dataframe(data):\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def tag_attrib(root):\n",
    "    data = []\n",
    "    for x in root:\n",
    "        y = x.tag\n",
    "        data.append(y)\n",
    "    return data\n",
    "\n",
    "def Description(root):\n",
    "    data = []\n",
    "    for x in root:\n",
    "        y = x.text\n",
    "        data.append(y)\n",
    "    return data\n",
    "\n",
    "\n",
    "def main(xml_file):\n",
    "    root = parse_xml(xml_file)\n",
    "    print(root)\n",
    "    data = extract_data(root)\n",
    "    print(data)\n",
    "    df = to_dataframe(data)\n",
    "    print(df)\n",
    "    ta = tag_attrib(root)\n",
    "    print(ta)\n",
    "    data_test = Description(root)\n",
    "    print(data_test)   \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main('Fares Data/Ross Travel_389/ROSS_144_Inbound_ADsgl_43ad4e3d-65a5-49b7-8a29-3dceda1d9905_638241799544014471.xml')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new file based on the Fares Data directory for saving all JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS Project/BODS/Nottingham City Transport Ltd_25 JSON/'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil, os\n",
    "\n",
    "def ig_f(dir, files):\n",
    "    return [f for f in files if os.path.isfile(os.path.join(dir, f))]\n",
    "\n",
    "SRC = 'C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS Project/BODS/Nottingham City Transport Ltd_25/'\n",
    "\n",
    "DEST = 'C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS Project/BODS/Nottingham City Transport Ltd_25 JSON/'\n",
    "\n",
    "shutil.copytree(SRC, DEST, ignore=ig_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all the xml from their zip files in 'Fare Data' folder and remove an .zip files afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, zipfile\n",
    "\n",
    "my_dir = 'C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS/Fares Data'\n",
    "\n",
    "for subdir, dirs, files in os.walk(my_dir):\n",
    "    for file in files:\n",
    "        #print os.path.join(subdir, file)\n",
    "        filepath = subdir + os.sep + file\n",
    "\n",
    "        if filepath.endswith(\".zip\"):\n",
    "            zip_ref = zipfile.ZipFile(filepath) # create zipfile object\n",
    "            zip_ref.extractall(subdir) # extract file to dir\n",
    "            zip_ref.close() # close file\n",
    "            os.remove(filepath) # delete zipped file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Scrape through the Fares Data Files after they've been converted from .zip to .xml\n",
    " - Check and see if there are any folders that have subfolders\n",
    " - grab a list of these folders\n",
    " - move any .xml files from these subfolders to the parent folder\n",
    " - delete the subfolders\n",
    " - Fares Data folder now ready for JSON conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from os import listdir, rmdir\n",
    "from shutil import move\n",
    "import pathlib\n",
    "\n",
    "rootdir = 'C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS/Fares Data/'\n",
    "\n",
    "\n",
    "def listdirs(rootdir):\n",
    "\n",
    "    list_of_subdir = []\n",
    "\n",
    "    for file in os.listdir(rootdir):\n",
    "        d = os.path.join(rootdir, file)\n",
    "        for file2 in os.listdir(d):\n",
    "            e = os.path.join(d, file2)\n",
    "            if os.path.isdir(e):\n",
    "                list_of_subdir.append(e)\n",
    "                #listdirs(e)\n",
    "    return list_of_subdir\n",
    "\n",
    "def move_xml(sublist):\n",
    "\n",
    "    for i in sublist:\n",
    "        path = pathlib.Path(str(i))\n",
    "        dest = path.parent\n",
    "        for filename in os.listdir(path):\n",
    "            new_src = str(path) + os.sep + str(filename)\n",
    "            new_dst = str(dest)\n",
    "            print(new_dst)\n",
    "            if new_src.endswith(\".xml\"):\n",
    "                move(new_src, new_dst)\n",
    "\n",
    "def del_subs(sublist):\n",
    "\n",
    "    for i in sublist:\n",
    "        os.rmdir(i)\n",
    "\n",
    " \n",
    "sublist = listdirs(rootdir)\n",
    "print(sublist)\n",
    "# move_xml(sublist)\n",
    "del_subs(sublist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XML to JSON testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmltodict, json\n",
    "\n",
    "with open('Fares Data/Ross Travel_389/ROSS_144_Inbound_ADsgl_43ad4e3d-65a5-49b7-8a29-3dceda1d9905_638241799544014471.xml','r') as test_file:\n",
    "    obj = xmltodict.parse(test_file.read())\n",
    "\n",
    "x = json.dumps(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, xmltodict, json\n",
    "\n",
    "SRC = 'C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS Project/BODS/Nottingham City Transport Ltd_25/Single Ticket'\n",
    "\n",
    "for subdir, dirs, files in os.walk(SRC):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        if filepath.endswith(\".xml\"):\n",
    "            with open(filepath,'r') as test_file:\n",
    "                obj = xmltodict.parse(test_file.read())\n",
    "                jd = json.dumps(obj, ensure_ascii=False, indent=4)\n",
    "                new_filepath = filepath.replace('Nottingham City Transport Ltd_25', 'Nottingham City Transport Ltd_25 JSON')\n",
    "                final_filepath = new_filepath.replace('.xml', '.json')\n",
    "                with open(final_filepath, \"w\", encoding='utf-8') as jf:\n",
    "                    jf.write(jd)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, zipfile\n",
    "\n",
    "my_dir = 'C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS/Fares Data JSON'\n",
    "\n",
    "for subdir, dirs, files in os.walk(my_dir):\n",
    "    for file in files:\n",
    "        #print os.path.join(subdir, file)\n",
    "        filepath = subdir + os.sep + file\n",
    "\n",
    "        if filepath.endswith(\".json\"):\n",
    "            os.remove(filepath) # delete zipped file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Quick SubScript to test out timetable data ###\n",
    "\n",
    "import os, xmltodict, json\n",
    "\n",
    "SRC = 'C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS/Arriva Donnington Timetable Data'\n",
    "\n",
    "for subdir, dirs, files in os.walk(SRC):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        if filepath.endswith(\".xml\"):\n",
    "            with open(filepath,'r') as test_file:\n",
    "                obj = xmltodict.parse(test_file.read())\n",
    "                jd = json.dumps(obj, ensure_ascii=False, indent=4)\n",
    "                new_filepath = filepath.replace('Arriva Donnington Timetable Data', 'Arriva JSON')\n",
    "                final_filepath = new_filepath.replace('.xml', '.json')\n",
    "                jf = open(final_filepath, \"w\", encoding='utf-8')\n",
    "                jf.write(jd)\n",
    "                jf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inital Python code for scraping through the Timetable JSON and extracting the nesscessary geo data for Michael to use.\n",
    "\n",
    "Error comes up as some JSON have different schemas, so code breaks when it runs into that error.\n",
    "\n",
    "Putting on hold for now until we have our meeting with Spencer qbout the best approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS Project/BODS/Ross Travel TT JSON\\ROSS_144_ROSSPB11354033144_20230403_-_1680691.json\n",
      "C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS Project/BODS/Ross Travel TT JSON\\ROSS_144_ROSSPB11354033144_20230403_-_1680692.json\n",
      "C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS Project/BODS/Ross Travel TT JSON\\ROSS_146_ROSSPB11354033146_20230403_-_1680689.json\n",
      "C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS Project/BODS/Ross Travel TT JSON\\ROSS_146_ROSSPB11354033146_20230403_-_1680690.json\n",
      "C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS Project/BODS/Ross Travel TT JSON\\ROSS_158_ROSSPB113540311158_20230403_-_1679615.json\n",
      "C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS Project/BODS/Ross Travel TT JSON\\ROSS_158_ROSSPB113540311158_20230403_-_1679616.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import natsort as ns\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "def dictionaries(json_dict):\n",
    "\n",
    "    '''\n",
    "    Function that grabs the appropraite dictionaries to pass through any of the remaining functions\n",
    "\n",
    "    Dictionary names are not passed through as a parameter for now for testing purposes\n",
    "\n",
    "    '''\n",
    "\n",
    "    stop_dict = json_dict['TransXChange']['StopPoints']['AnnotatedStopPointRef']\n",
    "    route_dict = json_dict['TransXChange']['RouteSections']['RouteSection']\n",
    "\n",
    "    return stop_dict, route_dict\n",
    "\n",
    "\n",
    "def stop_conversion(dict):\n",
    "\n",
    "    '''\n",
    "    Convert the stop route dictionary within timetable data and convert it to a DF\n",
    "\n",
    "    '''\n",
    "\n",
    "    df = pd.DataFrame.from_dict(dict)\n",
    "    temp_df = pd.json_normalize(df['Location'])\n",
    "    df = df.drop('Location', axis=1)\n",
    "    final_df = pd.concat([df, temp_df],axis=1)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "def route_conversion(dict):\n",
    "\n",
    "    '''\n",
    "    Convert the route dictionary within timetable data and convert it to a DF\n",
    "    \n",
    "    '''\n",
    "\n",
    "    df = pd.DataFrame.from_dict(dict)\n",
    "    final_route_df = []\n",
    "\n",
    "    for i in range(0, len(df)):\n",
    "        x = list(df['RouteLink'].iloc[i])\n",
    "        temp_df1 = pd.DataFrame.from_dict(x)\n",
    "        temp_df2 = pd.json_normalize(temp_df1['From'])\n",
    "        temp_df2.rename(columns={'StopPointRef': 'From'}, inplace=True)\n",
    "        temp_df3 = pd.json_normalize(temp_df1['To'])\n",
    "        temp_df3.rename(columns={'StopPointRef': 'To'}, inplace=True)\n",
    "        temp_df1 = temp_df1.drop(['From', 'To'], axis=1)\n",
    "        temp_df4 = pd.concat([temp_df1, temp_df2],axis=1)\n",
    "        final_df = pd.concat([temp_df4, temp_df3],axis=1)\n",
    "        final_df['Group @id'] = df.loc[i,'@id']\n",
    "        first_column = final_df.pop('Group @id')\n",
    "        final_df.insert(0, 'Group @id', first_column)\n",
    "        final_route_df.append(final_df)\n",
    "    \n",
    "    final_route_df = pd.concat(final_route_df).reset_index(drop=True)\n",
    "\n",
    "    return final_route_df\n",
    "\n",
    "def df_merge(route_df, stop_df):\n",
    "\n",
    "    '''\n",
    "    Merging the DF's a cleaning before exporting to CSV\n",
    "    \n",
    "    '''\n",
    "\n",
    "    temp_df1 = route_df.merge(stop_df, left_on='From', right_on='StopPointRef')\n",
    "    final_df = temp_df1.merge(stop_df, left_on='To', right_on='StopPointRef')\n",
    "    final_df = final_df[['Group @id', '@id', 'From', 'CommonName_x', 'Longitude_x', 'Latitude_x', 'To', 'CommonName_y', 'Longitude_y', 'Latitude_y']]\n",
    "    \n",
    "    final_df['@id'] = pd.Categorical(final_df['@id'], ordered=True, categories= ns.natsorted(final_df['@id'].unique()))\n",
    "    final_df = final_df.sort_values('@id')\n",
    "\n",
    "    final_df.rename(columns={'Group @id': 'Grouped Route IDs',\n",
    "                             '@id': 'Route ID',\n",
    "                             'From': 'From ID',\n",
    "                             'CommonName_x':'From - Common Name',\n",
    "                             'Longitude_x': 'From - Longitude',\n",
    "                             'Latitude_x': 'From - Latitude',\n",
    "                             'To': 'To ID',\n",
    "                             'CommonName_y':'To - Common Name',\n",
    "                             'Longitude_y': 'To - Longitude',\n",
    "                             'Latitude_y': 'To - Latitude',}, inplace=True)\n",
    "\n",
    "    final_df.reset_index(drop=True)\n",
    "    \n",
    "    return final_df\n",
    "    \n",
    "    \n",
    "def main(json_folder):\n",
    "\n",
    "    \n",
    "    for filename in os.listdir(json_folder):\n",
    "        filepath = os.path.join(json_folder, filename)\n",
    "        print(filepath)\n",
    "        with open(filepath) as test_json:\n",
    "            json_dict = json.load(test_json)\n",
    "            stop_dict, route_dict = dictionaries(json_dict)\n",
    "            stop_df = stop_conversion(stop_dict)\n",
    "            route_df = route_conversion(route_dict)\n",
    "            merged_df = df_merge(route_df, stop_df)\n",
    "            merged_df.to_csv(os.path.join('C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS Project/BODS/Ross Travel TT CSV/', (str(filename) + '.csv')))\n",
    "                        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main('C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS Project/BODS/Ross Travel TT JSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, xmltodict, json\n",
    "\n",
    "SRC = 'C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS Project/BODS/Ross Travel TT XML/'\n",
    "\n",
    "\n",
    "for subdir, dirs, files in os.walk(SRC):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        if filepath.endswith(\".xml\"):\n",
    "            with open(filepath,'r') as test_file:\n",
    "                obj = xmltodict.parse(test_file.read())\n",
    "                jd = json.dumps(obj, ensure_ascii=False, indent=4)\n",
    "                new_filepath = 'C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS Project/BODS/Ross Travel TT JSON/' + file\n",
    "                final_filepath = new_filepath.replace('.xml', '.json')\n",
    "                jf = open(final_filepath, \"w\", encoding='utf-8')\n",
    "                jf.write(jd)\n",
    "                jf.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 'C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS/Ross Travel JSON/ROSS_144_Inbound_ADsgl_43ad4e3d-65a5-49b7-8a29-3dceda1d9905_638241799544014471.json'\n",
    "\n",
    "with open(d) as test_json:\n",
    "    json_dict = json.load(test_json)\n",
    "\n",
    "x = json_dict['PublicationDelivery']['dataObjects']['CompositeFrame']\n",
    "df = pd.DataFrame.from_dict(x)\n",
    "\n",
    "x = df.loc[5]\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing some initial scraping with the Ross Travel JSON to try and understand the main information that we need to extract\n",
    "\n",
    "Once the main information is drawn, split the tables into the type of ticket they are being used for \n",
    "\n",
    "Do some more cleaning then attempt to join all of the data tables together into 1 big data table for Ross Travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this specific JSON file for Route 144 ADsgl, I think for an intitial POC, we can look at the top half of the file that contains a majority of the information relating to stop points, fare zones, routes, etc.\n",
    "\n",
    "The other half of the JSON file talks about properties of the route including things such as:\n",
    "\n",
    "- How the ticket was purchased\n",
    "- Where the ticket is located\n",
    "- what information determines the the type of passenger\n",
    "\n",
    "I think just focusing on getting a flat fare information for each route and stop is more benefical for the POC then we can add more details information when the time comes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, xmltodict, json, shutil\n",
    "\n",
    "SRC = 'C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS Project/BODS/Nottingham City Transport Ltd_25/Single Ticket/Inbound/Student'\n",
    "inbound = 'Inbound'\n",
    "outbound = 'Outbound'\n",
    "\n",
    "for subdir, dirs, files in os.walk(SRC):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        # if inbound in filepath:\n",
    "        #     new_filepath = 'C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS Project/BODS/Nottingham City Transport Ltd_25/Single Ticket/Inbound' + os.sep + file\n",
    "        #     shutil.move(filepath, new_filepath)\n",
    "        if outbound in filepath:\n",
    "            new_filepath = 'C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS Project/BODS/Nottingham City Transport Ltd_25/Single Ticket/Outbound' + os.sep + file\n",
    "            shutil.move(filepath, new_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, xmltodict, json, shutil\n",
    "\n",
    "SRC1 = 'C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS Project/BODS/Nottingham City Transport Ltd_25/Single Ticket/Inbound'\n",
    "SRC2 = 'C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS Project/BODS/Nottingham City Transport Ltd_25/Single Ticket/Outbound'\n",
    "adult = 'Adult'\n",
    "child = 'Child'\n",
    "student = 'Student'\n",
    "\n",
    "for subdir, dirs, files in os.walk(SRC2):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        if adult in filepath:\n",
    "            new_filepath = 'C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS Project/BODS/Nottingham City Transport Ltd_25/Single Ticket/Outbound/Adult' + os.sep + file\n",
    "            shutil.move(filepath, new_filepath)\n",
    "        elif child in filepath:\n",
    "            new_filepath = 'C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS Project/BODS/Nottingham City Transport Ltd_25/Single Ticket/Outbound/Child' + os.sep + file\n",
    "            shutil.move(filepath, new_filepath)\n",
    "        elif student in filepath:\n",
    "            new_filepath = 'C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS Project/BODS/Nottingham City Transport Ltd_25/Single Ticket/Outbound/Student' + os.sep + file\n",
    "            shutil.move(filepath, new_filepath)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, 'gb_GB')\n",
    "\n",
    "test_json = open('C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS Project/BODS/Nottingham City Transport Ltd_25 JSON/Single Ticket/Inbound/Adult/SingleTicket - NCTR - 1 - Inbound - AdultPassengerSingleTicket.json')\n",
    "data = json.load(test_json)\n",
    "\n",
    "line_id = data['PublicationDelivery']['dataObjects']['CompositeFrame'][0]['frames']['ServiceFrame']['lines']['Line']['@id']\n",
    "line_name = data['PublicationDelivery']['dataObjects']['CompositeFrame'][0]['frames']['ServiceFrame']['lines']['Line']['Name']\n",
    "line_public_code = data['PublicationDelivery']['dataObjects']['CompositeFrame'][0]['frames']['ServiceFrame']['lines']['Line']['PublicCode']\n",
    "\n",
    "scheduled_stop_points = data['PublicationDelivery']['dataObjects']['CompositeFrame'][0]['frames']['ServiceFrame']['scheduledStopPoints']['ScheduledStopPoint']\n",
    "\n",
    "route_stop_ids = [i['@id'] for i in scheduled_stop_points]\n",
    "route_stop_names = [i['Name'] for i in scheduled_stop_points]\n",
    "route_stop_topog_refs = [i['TopographicPlaceView']['TopographicPlaceRef']['@ref'] for i in scheduled_stop_points]\n",
    "route_stop_topog_names = [i['TopographicPlaceView']['Name'] for i in scheduled_stop_points]\n",
    "\n",
    "\n",
    "fare_zone_points = data['PublicationDelivery']['dataObjects']['CompositeFrame'][0]['frames']['FareFrame'][0]['fareZones']['FareZone']\n",
    "\n",
    "fare_zone_ids = [i['@id'] for i in fare_zone_points]\n",
    "fare_zone_names = [i['Name'] for i in fare_zone_points]\n",
    "fare_zone_stop_point_ref = [i['members']['ScheduledStopPointRef']['@ref'] for i in fare_zone_points]\n",
    "fare_zone_stop_point_text = [i['members']['ScheduledStopPointRef']['#text'] for i in fare_zone_points]\n",
    "\n",
    "tariff = data['PublicationDelivery']['dataObjects']['CompositeFrame'][0]['frames']['FareFrame'][1]['tariffs']['Tariff']['Name']\n",
    "tariff_basis = data['PublicationDelivery']['dataObjects']['CompositeFrame'][0]['frames']['FareFrame'][1]['tariffs']['Tariff']['TypeOfTariffRef']['@ref']\n",
    "\n",
    "distance_matrix_element = data['PublicationDelivery']['dataObjects']['CompositeFrame'][0]['frames']['FareFrame'][1]['tariffs']['Tariff']['fareStructureElements']['FareStructureElement'][0]['distanceMatrixElements']['DistanceMatrixElement']\n",
    "\n",
    "fare_tariff_id = [i['@id'] for i in distance_matrix_element]\n",
    "fare_tariff_price_band = [i['priceGroups']['PriceGroupRef']['@ref'] for i in distance_matrix_element]\n",
    "fare_tariff_start_zone = [i['StartTariffZoneRef']['@ref'] for i in distance_matrix_element]\n",
    "fare_tariff_end_zone = [i['EndTariffZoneRef']['@ref'] for i in distance_matrix_element]\n",
    "\n",
    "\n",
    "tariff_dict = {\n",
    "    'Tariff ID':fare_tariff_id,\n",
    "    'Tariff Start Zone': fare_tariff_start_zone,\n",
    "    'Tariff End Zone': fare_tariff_end_zone,\n",
    "    'Tariff Price Band': fare_tariff_price_band\n",
    "}\n",
    "\n",
    "fare_zone_dict = {\n",
    "    'Fare Zone ID': fare_zone_ids,\n",
    "    'Fare Zone Names': fare_zone_names,\n",
    "    'Fare Zone Stop Reference': fare_zone_stop_point_ref\n",
    "}\n",
    "\n",
    "route_stop_dict = {\n",
    "    'Stop ID': route_stop_ids,\n",
    "    'Stop Name': route_stop_names\n",
    "    }\n",
    "\n",
    "tariff_df = pd.DataFrame(tariff_dict)\n",
    "fare_zone_df = pd.DataFrame(fare_zone_dict)\n",
    "route_df = pd.DataFrame(route_stop_dict)\n",
    "\n",
    "route_stops_df = fare_zone_df.merge(route_df, left_on='Fare Zone Stop Reference', right_on='Stop ID')\n",
    "route_stops_df = route_stops_df.drop(['Stop ID'], axis=1)\n",
    "\n",
    "tariff_sz_df = tariff_df.merge(route_stops_df, left_on='Tariff Start Zone', right_on='Fare Zone ID')\n",
    "cols = ['Tariff ID', 'Tariff Start Zone','Fare Zone Stop Reference', 'Stop Name','Tariff End Zone', 'Tariff Price Band']\n",
    "tariff_sz_df = tariff_sz_df[cols]\n",
    "\n",
    "tariff_ez_df = tariff_df.merge(route_stops_df, left_on='Tariff End Zone', right_on='Fare Zone ID', how='left')\n",
    "cols = ['Tariff End Zone','Fare Zone Stop Reference', 'Stop Name']\n",
    "tariff_ez_df = tariff_ez_df[cols]\n",
    "tariff_ez_df.rename({'Stop Name': 'Stop Name EZ',\n",
    "                     'Fare Zone Stop Reference': 'Fare Zone Stop Reference EZ'},axis=1, inplace=True)\n",
    "\n",
    "tariff_combined_df = pd.concat([tariff_sz_df, tariff_ez_df], axis=1, join='inner')\n",
    "\n",
    "money = []\n",
    "for i in tariff_combined_df['Tariff Price Band']:\n",
    "    x = re.sub('\\D', '', i)\n",
    "    x = locale.currency(int(x)/100)\n",
    "    x = x.replace('+','')\n",
    "    money.append(x)\n",
    "tariff_combined_df['Route Cost (£)'] = money\n",
    "\n",
    "tariff_combined_df = tariff_combined_df.drop(['Tariff Start Zone', 'Tariff End Zone', 'Tariff Price Band'], axis=1)\n",
    "\n",
    "tariff_combined_df.insert(0, column='Public Code', value=line_public_code)\n",
    "tariff_combined_df.insert(0, column='Name', value=line_name)\n",
    "tariff_combined_df.insert(0, column='ID', value=line_id)\n",
    "\n",
    "tariff_combined_df['Fare Zone Stop Reference'] = tariff_combined_df['Fare Zone Stop Reference'].replace('atco:', '', regex=True)\n",
    "tariff_combined_df['Fare Zone Stop Reference EZ'] = tariff_combined_df['Fare Zone Stop Reference EZ'].replace('atco:', '', regex=True)\n",
    "\n",
    "tariff_combined_df.rename({'ID': 'Line ID',\n",
    "                           'Name': 'Line Name',\n",
    "                           'Public Code':'Line Public Code',\n",
    "                           'Tariff ID': 'Tariff Route ID',\n",
    "                           'Stop Name':'Tariff Route Start Name',\n",
    "                           'Fare Zone Stop Reference':'Tariff Route Start ID',\n",
    "                           'Stop Name EZ':'Tariff Route End Name',\n",
    "                           'Fare Zone Stop Reference EZ':'Tariff Route End ID',\n",
    "                           'Route Cost (£)':'Tariff Route Cost (£)'},axis=1, inplace=True)\n",
    "\n",
    "cols = ['Line ID', 'Line Name', 'Line Public Code', 'Tariff Route ID', 'Tariff Route Start Name', 'Tariff Route Start ID', 'Tariff Route End Name', 'Tariff Route End ID', 'Tariff Route Cost (£)']\n",
    "tariff_combined_df = tariff_combined_df[cols]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#df7.to_csv('NCTR_Single_Inbound_Adult_Route_1.csv',index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:', 'Users', 'rosshamilton', 'OneDrive - KPMG', 'Documents', 'BODS Project', 'BODS', 'Nottingham City Transport Ltd_25 JSON', 'Single Ticket', 'Inbound', 'Adult']\n",
      "NCTR_SingleTicket_Inbound_Adult_Routes_&_Fares.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "string = 'C:/Users/rosshamilton/OneDrive - KPMG/Documents/BODS Project/BODS/Nottingham City Transport Ltd_25 JSON/Single Ticket/Inbound/Adult'\n",
    "\n",
    "elements = string.split('/')\n",
    "\n",
    "print(elements)\n",
    "\n",
    "new_string = 'NCTR_'+ elements[8] + '_' + elements[9] + '_' + elements[10] + '_Routes_&_Fares.csv'\n",
    "new_string  = new_string.replace(' ','')\n",
    "print(new_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BODS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
